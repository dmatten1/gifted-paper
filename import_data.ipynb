{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from dateutil.parser import parse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Load and clean ===\n",
    "source_path = \"../ncerdc-data/\"\n",
    "prefix = \"curtest\"  \n",
    "\n",
    "sas_files = [f for f in os.listdir(source_path) if f.startswith(prefix) and f.endswith(\".sas7bdat\")]\n",
    "df_list = [pd.read_sas(os.path.join(source_path, f), format=\"sas7bdat\", encoding=\"latin1\") for f in sas_files]\n",
    "big_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Standardize column names\n",
    "caps_cols = [col for col in big_df.columns if col.isupper()]\n",
    "for col in caps_cols:\n",
    "    lower_col = col.lower()\n",
    "    if lower_col in big_df.columns:\n",
    "        big_df[lower_col] = big_df[lower_col].combine_first(big_df[col])\n",
    "        big_df = big_df.drop(columns=[col])\n",
    "    else:\n",
    "        big_df = big_df.rename(columns={col: lower_col})\n",
    "\n",
    "big_df = big_df[['year','mastid','test_id','score']]\n",
    "\n",
    "# Clean scores\n",
    "big_df[\"score\"] = big_df[\"score\"].replace(\"NULL\", np.nan)\n",
    "big_df.dropna(subset=[\"score\"], inplace=True)\n",
    "big_df[\"score\"] = pd.to_numeric(big_df[\"score\"], errors=\"coerce\").astype(\"float\")\n",
    "\n",
    "# === Handle ACT differently ===\n",
    "# Flag ACT tests\n",
    "is_act = big_df[\"test_id\"].str.startswith(\"AC\")\n",
    "\n",
    "# Percentiles for non-ACT\n",
    "big_df.loc[~is_act, \"score_value\"] = (\n",
    "    big_df.loc[~is_act]\n",
    "    .groupby([\"year\", \"test_id\"])[\"score\"]\n",
    "    .rank(pct=True) * 100\n",
    ")\n",
    "\n",
    "# Raw scores for ACT\n",
    "big_df.loc[is_act, \"score_value\"] = big_df.loc[is_act, \"score\"]\n",
    "\n",
    "# Drop unnecessary cols\n",
    "big_df = big_df.drop(columns=[\"score\", \"year\"])\n",
    "big_df.dropna(subset=[\"score_value\"], inplace=True)\n",
    "\n",
    "# === Pivot ===\n",
    "pivot_df = big_df.pivot_table(\n",
    "    index=\"mastid\",\n",
    "    columns=\"test_id\",\n",
    "    values=\"score_value\",\n",
    "    aggfunc=\"first\"\n",
    ").reset_index()\n",
    "\n",
    "# Drop unwanted test_id groups\n",
    "pivot_df = pivot_df.loc[:, ~pivot_df.columns.str.startswith((\"X\", \"W\", \"U\", \"G\", \"C\", \"P\"))]\n",
    "pivot_df = pivot_df.drop(columns=['RD3A','RD3B','RD3R','ALG1','ALG2','ENG2','ENGL','MTH3'], errors=\"ignore\")\n",
    "\n",
    "# === Split ACT vs others ===\n",
    "a_cols = [c for c in pivot_df.columns if c.startswith(\"AC\")] + [\"mastid\"]\n",
    "act = pivot_df[a_cols]\n",
    "\n",
    "other_cols = [c for c in pivot_df.columns if c not in a_cols]\n",
    "df_other = pivot_df[[\"mastid\"] + other_cols]\n",
    "\n",
    "# === Save ===\n",
    "df_other.to_csv(\"data/interim/curtest_master.csv\", index=False)\n",
    "act.to_csv(\"data/interim/act.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "masterbuild is such a pain to do again, exercise for the reader\n",
    "lea, gender, ethnicity, eds, aig, grade and year are contained in masterbuild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sat\n",
    "sat = pd.DataFrame()\n",
    "for year in range(18, 23):  # 23 because range is exclusive at the end\n",
    "    filename = f\"../ncerdc-data/collegeboard{year}pub.sas7bdat\"\n",
    "    try:\n",
    "        # Read CSV file\n",
    "        df = pd.read_sas(filename)\n",
    "        \n",
    "        # Keep only desired columns\n",
    "        cols = [c for c in df.columns if c.startswith(\"SAT_Total_Score_HC\") or c == \"mastid\"]\n",
    "\n",
    "        df = df[cols]        \n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        # Append to the main DataFrame\n",
    "        sat = pd.concat([sat, df], ignore_index=True)\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {filename} not found. Skipping.\")\n",
    "    \n",
    "\n",
    "# Optional: Display or save the combined DataFrame\n",
    "# sat.to_csv(\"combined_sat_data.csv\", index=False)\n",
    "# Keep the highest SAT_Total_Score_HC for each mastid\n",
    "sat = sat.sort_values('SAT_Total_Score_HC', ascending=False).drop_duplicates(subset='mastid', keep='first')\n",
    "\n",
    "sat.to_csv(\"data/interim/sat.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"../ncerdc-data\"  # Change this if your files are in a different directory\n",
    "\n",
    "\n",
    "# Loop through files in the directory\n",
    "for filename in os.listdir(folder_path):\n",
    "    match = re.match(r\"gpa(\\d{4})\\.sas7bdat\", filename)\n",
    "    if match:\n",
    "        year = match.group(1)\n",
    "        var_name = f\"gpa{year}\"\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        df = pd.read_sas(file_path)\n",
    "        globals()[var_name] = df\n",
    "        \n",
    "gpa_vars = [var for var in globals() if var.startswith(\"gpa\") and isinstance(globals()[var], pd.DataFrame) and var != \"gpa_master\"]\n",
    "\n",
    "\n",
    "gpa_master = pd.concat([globals()[var] for var in gpa_vars], ignore_index=True)\n",
    "gpa_master = gpa_master.map(lambda x: x[2:-1] if isinstance(x, str) and x.startswith('b') else x)\n",
    "\n",
    "\n",
    "# Combine the columns, prioritizing non-null values in 'bound_for'\n",
    "gpa_master['bound_for_combined'] = gpa_master['bound_for'].combine_first(gpa_master['BOUND_FOR'])\n",
    "\n",
    "# Drop the original columns\n",
    "gpa_master.drop(columns=['bound_for', 'BOUND_FOR'], inplace=True)\n",
    "\n",
    "# Rename to a standard name (optional)\n",
    "gpa_master.rename(columns={'bound_for_combined': 'bound_for'}, inplace=True)\n",
    "\n",
    "\n",
    "gpa_master.drop(columns=['DIPLOMA_MET','UNWEIGHTED_RANK_DATE','WEIGHTED_RANK_DATE','diploma_type','DIPLOMA_TYPE'], inplace=True)\n",
    "\n",
    "\n",
    "# def fast_parse_dates(series):\n",
    "#     # Try general parse first (fastest)\n",
    "#     parsed = pd.to_datetime(series, errors='coerce')\n",
    "\n",
    "#     # Optionally: fallback to common format if still missing\n",
    "#     fallback = pd.to_datetime(series, format='%m/%d/%Y', errors='coerce')\n",
    "#     parsed = parsed.fillna(fallback)\n",
    "\n",
    "#     return parsed.dt.year\n",
    "\n",
    "\n",
    "# gpa_master['entry_year'] = fast_parse_dates(gpa_master['NINTHGRADEENTRY'])\n",
    "# gpa_master['grad_year'] = fast_parse_dates(gpa_master['DIPLOMA_ISSUED'])\n",
    "\n",
    "def clean_bytes(df):\n",
    "    \"\"\"\n",
    "    Convert byte-string columns (like b'ABC') into normal Python strings.\n",
    "    Applies only to object or string dtypes.\n",
    "    \"\"\"\n",
    "    for col in df.select_dtypes(include=[\"object\", \"string\"]):\n",
    "        df[col] = df[col].apply(\n",
    "            lambda x: x.decode(\"utf-8\").strip() if isinstance(x, bytes) else x\n",
    "        )\n",
    "    return df\n",
    "\n",
    "gpa_master = gpa_master[['mastid','bound_for','gpa_unweighted','gpa_weighted']]\n",
    "gpa_master = clean_bytes(gpa_master)\n",
    "gpa_master = gpa_master[(gpa_master['gpa_unweighted'] <= 4) & (gpa_master['gpa_weighted'] <= 6)]\n",
    "gpa_master.to_csv(\"data/interim/gpa_master.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
